{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Bianca Joy Benedictos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section: S16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will extend create a 2-layer neural network using TensorFlow. \n",
    "\n",
    "**Note: there is a jump from how scikit and TensorFlow train their models.** It might be helpful to think that TensorFlow sets the design of the computation graph first, then starts a session to \"feed\" the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* Read each cell and implement the TODOs sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Answer all the markdown/text cells with \"A: \" on them. The answer must strictly consume one line only.\n",
    "* You are expected to search how to some functions work on the Internet or via the docs. \n",
    "* You may add new cells for \"scrap work\".\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You are expected to understand the data set loading and processing separately from this class.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "# Fix the seed of the random number \n",
    "# generator so that your results will match ours\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following experiments, we will be creating a 2 layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qmlQoAG.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be using TensorFlow. Please check out https://learningtensorflow.com/getting_started/ for a basics guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first set the size of the layers of our network. We will use these variables later on in our code. (This is step **(a)**, see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we should be getting the size of our input and output layer based on our data (the size of the hidden layer/s is/are completely up to us). But in this experiment, we will be creating our own dummy data with 10 features and 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 10 # number of features/ size of the input layer (length of W[1]) (D)\n",
    "n_hidden = 10 # number of neurons in the hidden layer (length of W[2])\n",
    "n_output = 2 # number of classes/labels (K)\n",
    "n_instances = 100 # number of instances (N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random dataset (dummy data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536e+01 -6.11756414e+00 -5.28171752e+00 -1.07296862e+01\n",
      "   8.65407629e+00 -2.30153870e+01  1.74481176e+01 -7.61206901e+00\n",
      "   3.19039096e+00 -2.49370375e+00]\n",
      " [ 1.46210794e+01 -2.06014071e+01 -3.22417204e+00 -3.84054355e+00\n",
      "   1.13376944e+01 -1.09989127e+01 -1.72428208e+00 -8.77858418e+00\n",
      "   4.22137467e-01  5.82815214e+00]\n",
      " [-1.10061918e+01  1.14472371e+01  9.01590721e+00  5.02494339e+00\n",
      "   9.00855949e+00 -6.83727859e+00 -1.22890226e+00 -9.35769434e+00\n",
      "  -2.67888080e+00  5.30355467e+00]\n",
      " [-6.91660752e+00 -3.96753527e+00 -6.87172700e+00 -8.45205641e+00\n",
      "  -6.71246131e+00 -1.26645989e-01 -1.11731035e+01  2.34415698e+00\n",
      "   1.65980218e+01  7.42044161e+00]\n",
      " [-1.91835552e+00 -8.87628964e+00 -7.47158294e+00  1.69245460e+01\n",
      "   5.08077548e-01 -6.36995647e+00  1.90915485e+00  2.10025514e+01\n",
      "   1.20158952e+00  6.17203110e+00]\n",
      " [ 3.00170320e+00 -3.52249846e+00 -1.14251820e+01 -3.49342722e+00\n",
      "  -2.08894233e+00  5.86623191e+00  8.38983414e+00  9.31102081e+00\n",
      "   2.85587325e+00  8.85141164e+00]\n",
      " [-7.54397941e+00  1.25286816e+01  5.12929820e+00 -2.98092835e+00\n",
      "   4.88518147e+00 -7.55717130e-01  1.13162939e+01  1.51981682e+01\n",
      "   2.18557541e+01 -1.39649634e+01]\n",
      " [-1.44411381e+01 -5.04465863e+00  1.60037069e+00  8.76168921e+00\n",
      "   3.15634947e+00 -2.02220122e+01 -3.06204013e+00  8.27974643e+00\n",
      "   2.30094735e+00  7.62011180e+00]\n",
      " [-2.22328143e+00 -2.00758069e+00  1.86561391e+00  4.10051647e+00\n",
      "   1.98299720e+00  1.19008646e+00 -6.70662286e+00  3.77563786e+00\n",
      "   1.21821271e+00  1.12948391e+01]\n",
      " [ 1.19891788e+01  1.85156417e+00 -3.75284950e+00 -6.38730407e+00\n",
      "   4.23494354e+00  7.73400683e-01 -3.43853676e+00  4.35968568e-01\n",
      "  -6.20000844e+00  6.98032034e+00]\n",
      " [-4.47128565e+00  1.22450770e+01  4.03491642e+00  5.93578523e+00\n",
      "  -1.09491185e+01  1.69382433e+00  7.40556451e+00 -9.53700602e+00\n",
      "  -2.66218506e+00  3.26145467e-01]\n",
      " [-1.37311732e+01  3.15159392e+00  8.46160648e+00 -8.59515941e+00\n",
      "   3.50545979e+00 -1.31228341e+01 -3.86955093e-01 -1.61577235e+01\n",
      "   1.12141771e+01  4.08900538e+00]\n",
      " [-2.46169559e-01 -7.75161619e+00  1.27375593e+01  1.96710175e+01\n",
      "  -1.85798186e+01  1.23616403e+01  1.62765075e+01  3.38011697e+00\n",
      "  -1.19926803e+01  8.63345318e+00]\n",
      " [-1.80920302e+00 -6.03920628e+00 -1.23005814e+01  5.50537496e+00\n",
      "   7.92806866e+00 -6.23530730e+00  5.20576337e+00 -1.14434139e+01\n",
      "   8.01861032e+00  4.65672984e-01]\n",
      " [-1.86569772e+00 -1.01745873e+00  8.68886157e+00  7.50411640e+00\n",
      "   5.29465324e+00  1.37701210e+00  7.78211279e-01  6.18380262e+00\n",
      "   2.32494559e+00  6.82551407e+00]\n",
      " [-3.10116774e+00 -2.43483776e+01  1.03882460e+01  2.18697965e+01\n",
      "   4.41364444e+00 -1.00155233e+00 -1.36444744e+00 -1.19054188e+00\n",
      "   1.74094083e-01 -1.12201873e+01]\n",
      " [-5.17094458e+00 -9.97026828e+00  2.48799161e+00 -2.96641152e+00\n",
      "   4.95211324e+00 -1.74703160e+00  9.86335188e+00  2.13533901e+00\n",
      "   2.19069973e+01 -1.89636092e+01]\n",
      " [-6.46916688e+00  9.01486892e+00  2.52832571e+01 -2.48634778e+00\n",
      "   4.36689932e-01 -2.26314243e+00  1.33145711e+01 -2.87307863e+00\n",
      "   6.80069840e+00 -3.19801599e+00]\n",
      " [-1.27255876e+01  3.13547720e+00  5.03184813e+00  1.29322588e+01\n",
      "  -1.10447026e+00 -6.17362064e+00  5.62761097e+00  2.40737092e+00\n",
      "   2.80665077e+00 -7.31127037e-01]\n",
      " [ 1.16033857e+01  3.69492716e+00  1.90465871e+01  1.11105670e+01\n",
      "   6.59049796e+00 -1.62743834e+01  6.02319280e+00  4.20282204e+00\n",
      "   8.10951673e+00  1.04444209e+01]\n",
      " [-4.00878192e+00  8.24005618e+00 -5.62305431e+00  1.95487808e+01\n",
      "  -1.33195167e+01 -1.76068856e+01 -1.65072127e+01 -8.90555584e+00\n",
      "  -1.11911540e+01  1.95607890e+01]\n",
      " [-3.26499498e+00 -1.34267579e+01  1.11438298e+01 -5.86523939e+00\n",
      "  -1.23685338e+01  8.75838928e+00  6.23362177e+00 -4.34956683e+00\n",
      "   1.40754000e+01  1.29101580e+00]\n",
      " [ 1.61694960e+01  5.02740882e+00  1.55880554e+01  1.09402696e+00\n",
      "  -1.21974440e+01  2.44936865e+01 -5.45774168e+00 -1.98837863e+00\n",
      "  -7.00398505e+00 -2.03394449e+00]\n",
      " [ 2.42669441e+00  2.01830179e+00  6.61020288e+00  1.79215821e+01\n",
      "  -1.20464572e+00 -1.23312074e+01 -1.18231813e+01 -6.65754518e+00\n",
      "  -1.67419581e+01  8.25029824e+00]\n",
      " [-4.98213564e+00 -3.10984978e+00 -1.89148284e-02 -1.39662042e+01\n",
      "  -8.61316361e+00  6.74711526e+00  6.18539131e+00 -4.43171931e+00\n",
      "   1.81053491e+01 -1.30572692e+01]\n",
      " [-3.44987210e+00 -2.30839743e+00 -2.79308500e+01  1.93752881e+01\n",
      "   3.66332015e+00 -1.04458938e+01  2.05117344e+01  5.85662000e+00\n",
      "   4.29526140e+00 -6.06998398e+00]\n",
      " [ 1.06222724e+00 -1.52568032e+01  7.95026094e+00 -3.74438319e+00\n",
      "   1.34048197e+00  1.20205486e+01  2.84748111e+00  2.62467445e+00\n",
      "   2.76499305e+00 -7.33271604e+00]\n",
      " [ 8.36004719e+00  1.54335911e+01  7.58805660e+00  8.84908814e+00\n",
      "  -8.77281519e+00 -8.67787223e+00 -1.44087602e+01  1.23225307e+01\n",
      "  -2.54179868e+00  1.39984394e+01]\n",
      " [-7.81911683e+00 -4.37508983e+00  9.54250872e-01  9.21450069e+00\n",
      "   6.07501958e-01  2.11124755e+00  1.65275673e-01  1.77187720e+00\n",
      "  -1.11647002e+01  8.09271010e-01]\n",
      " [-1.86578994e+00 -5.68244809e-01  4.92336556e+00 -6.80678141e+00\n",
      "  -8.45080274e-01 -2.97361883e+00  4.17302005e+00  7.84770651e+00\n",
      "  -9.55425262e+00  5.85910431e+00]\n",
      " [ 2.06578332e+01 -1.47115693e+01 -8.30171895e+00 -8.80577600e+00\n",
      "  -2.79097722e+00  1.62284909e+01  1.33526763e-01 -6.94693595e+00\n",
      "   6.21803504e+00 -5.99804531e+00]\n",
      " [ 1.12341216e+01  3.05267040e+00  1.38877940e+01 -6.61344243e+00\n",
      "   3.03085711e+01  8.24584625e+00  6.54580153e+00 -5.11884476e-01\n",
      "  -7.25597119e+00 -8.67768678e+00]\n",
      " [-1.35977326e+00 -7.97269785e+00  2.82675712e+00 -8.26097432e+00\n",
      "   6.21082701e+00  9.56121704e+00 -7.05840507e+00  1.19268607e+01\n",
      "  -2.37941936e+00  1.15528789e+01]\n",
      " [ 4.38166347e+00  1.12232832e+01 -9.97019796e+00 -1.06793987e+00\n",
      "   1.45142926e+01 -6.18036848e+00 -2.03720123e+01 -1.94258918e+01\n",
      "  -2.50644065e+01 -2.11416392e+01]\n",
      " [-4.11639163e+00  1.27852808e+01 -4.42229280e+00  3.23527354e+00\n",
      "  -1.09991490e+00  8.54894544e-02 -1.68198840e+00 -1.74180344e+00\n",
      "   4.61164100e+00 -1.17598267e+01]\n",
      " [ 1.01012718e+01  9.20017933e+00 -1.95057341e+00  8.05393424e+00\n",
      "  -7.01344426e+00 -5.37223024e+00  1.56263850e+00 -1.90221025e+00\n",
      "  -4.48738033e+00 -6.72448039e+00]\n",
      " [-5.57494722e+00  9.39168744e+00 -1.94332341e+01  3.52494364e+00\n",
      "  -2.36436952e+00  7.27813500e+00  5.15073614e+00 -2.78253447e+01\n",
      "   5.84646610e+00  3.24274243e+00]\n",
      " [ 2.18628366e-01 -4.68673816e+00  8.53281222e+00 -4.13029310e+00\n",
      "   1.83471763e+01  5.64382855e+00  2.13782807e+01 -7.85533997e+00\n",
      "  -1.75592564e+01  7.14789597e+00]\n",
      " [ 8.52704062e+00  3.53600971e-01 -1.53879325e+01 -4.47895185e+00\n",
      "   6.17985534e+00 -1.84176326e+00 -1.15985185e+00 -1.75458969e+00\n",
      "  -9.33914656e+00 -5.33020326e+00]\n",
      " [-1.42655542e+01  1.76795995e+01 -4.75372875e+00  4.77610182e+00\n",
      "  -1.02188594e+01  7.94528240e+00 -1.87316098e+01  9.20615118e+00\n",
      "  -3.53679249e-01  2.11060505e+01]\n",
      " [-1.30653407e+01  7.63804802e-01  3.67231814e+00  1.23289919e+01\n",
      "  -4.22856961e+00  8.64644065e-01 -2.14246673e+01 -8.30168864e+00\n",
      "   4.51615951e+00  1.10417433e+01]\n",
      " [-2.81736269e+00  2.05635552e+01  1.76024923e+01 -6.06524918e-01\n",
      "  -2.41350300e+01 -1.77756638e+01 -7.77858827e+00  1.11584111e+01\n",
      "   3.10272288e+00 -2.09424782e+01]\n",
      " [-2.28765829e+00  1.61336137e+01 -3.74804687e+00 -7.49969617e+00\n",
      "   2.05462410e+01  5.34095368e-01 -4.79157099e+00  3.50167159e+00\n",
      "   1.71647264e-01 -4.29142278e+00]\n",
      " [ 1.20845633e+01  1.11570180e+01  8.40861558e+00 -1.02887218e+00\n",
      "   1.14690038e+01 -4.97025792e-01  4.66643267e+00  1.03368687e+01\n",
      "   8.08844360e+00  1.78975468e+01]\n",
      " [ 4.51284016e+00 -1.68405999e+01 -1.16017010e+01  1.35010682e+01\n",
      "  -3.31283170e+00  3.86539145e+00 -8.51455657e+00  1.00088142e+01\n",
      "  -3.84832249e+00  1.45810824e+01]\n",
      " [-5.32234021e+00  1.11813340e+01  6.74396105e+00 -7.22391905e+00\n",
      "   1.09899633e+01 -9.01634490e+00 -8.22467189e+00  7.21711292e+00\n",
      "  -6.25342001e+00 -5.93843067e+00]\n",
      " [-3.43900709e+00 -1.00016919e+01  1.04499441e+01  6.08514698e+00\n",
      "  -6.93286967e-01 -1.08392067e+00  4.50155513e+00  1.76533510e+01\n",
      "   8.70969803e+00 -5.08457134e+00]\n",
      " [ 7.77419205e+00 -1.18771172e+00 -1.98998184e+00  1.86647138e+01\n",
      "  -4.18937898e+00 -4.79184915e+00 -1.95210529e+01 -1.40232915e+01\n",
      "   4.51122939e+00 -6.94920901e+00]\n",
      " [ 5.15413802e+00 -1.11487105e+01 -7.67309826e+00  6.74570707e+00\n",
      "   1.46089238e+01  5.92472801e+00  1.19783084e+01  1.70459417e+01\n",
      "   1.04008915e+01 -9.18440038e+00]\n",
      " [-1.05344713e+00  6.30195671e+00 -4.14846901e+00  4.51946037e+00\n",
      "  -1.57915629e+01 -8.28627979e+00  5.28879746e+00 -2.23708651e+01\n",
      "  -1.10771250e+01 -1.77183179e-01]\n",
      " [-1.71939447e+01  5.71209961e-01 -7.99547491e+00 -2.91594596e+00\n",
      "  -2.58982853e+00  1.89293198e+00 -5.63788735e+00  8.96864073e-01\n",
      "  -6.01156801e+00  5.56073510e+00]\n",
      " [ 1.69380911e+01  1.96869779e+00  1.69869255e+00 -1.16400797e+01\n",
      "   6.93366226e+00 -7.58067329e+00 -8.08847196e+00  5.57439453e+00\n",
      "   1.81038744e+00  1.10717545e+01]\n",
      " [ 1.44287693e+01 -5.39681562e+00  1.28376990e+00  1.76041518e+01\n",
      "   9.66539250e+00  7.13049050e+00  1.30620607e+01 -6.04602969e+00\n",
      "   6.36583409e+00  1.40925339e+01]\n",
      " [ 1.62091229e+01 -8.06184817e+00 -2.51674208e+00  3.82715174e+00\n",
      "  -2.88997343e+00 -3.91816240e+00  6.84001328e+00 -3.53409983e+00\n",
      "  -1.78791289e+01  3.61847316e+00]\n",
      " [-4.24492791e+00 -7.31530982e+00 -1.56573815e+01  1.01382247e+01\n",
      "  -2.22711263e+01 -1.69933360e+01 -2.75846063e+00  1.22895559e+01\n",
      "   1.30970591e+01 -1.15498263e+01]\n",
      " [-1.77632196e+00 -1.51045638e+01  1.01120706e+01 -1.47656266e+01\n",
      "  -1.43195745e+00  1.03298378e+01 -2.22414029e+00  1.47016034e+01\n",
      "  -8.70008223e+00  3.69190470e+00]\n",
      " [ 8.53282186e+00 -1.39711730e+00  1.38631426e+01  5.48129585e+00\n",
      "  -1.63744959e+01  3.95860270e+01  6.48643644e+00  1.07343294e+00\n",
      "  -1.39881282e+01  8.17678188e-01]\n",
      " [-4.59942831e+00  6.44353666e+00  3.71670291e+00  1.85300949e+01\n",
      "   1.42251373e+00  5.13505480e+00  3.72456852e+00 -1.48489803e+00\n",
      "  -1.83400197e+00  1.10100020e+01]\n",
      " [ 7.80027135e+00 -6.29441604e+00 -1.11343610e+01 -6.74100249e-01\n",
      "   1.16143998e+01 -2.75293863e-01  1.74643509e+01 -7.75070287e+00\n",
      "   1.41640538e+00 -2.51630386e+01]\n",
      " [-5.95667881e+00 -3.09121319e+00  5.10937774e+00  1.71066184e+01\n",
      "   3.49435894e-01  1.45391758e+01  6.61681076e+00  9.86352180e+00\n",
      "  -4.66154857e+00  1.38499134e+01]\n",
      " [-1.07296428e+01  4.95158611e+00 -9.52062101e+00 -5.18145552e+00\n",
      "  -1.46140360e+01 -5.16347909e+00  3.51116897e+00 -6.87704631e-01\n",
      "  -1.34776494e+01  1.47073986e+01]\n",
      " [ 3.37220938e+00  1.00806543e+01  7.85226920e+00 -6.64867767e+00\n",
      "  -1.94504696e+01 -9.15424368e+00  1.22515585e+01 -1.05354607e+01\n",
      "   8.16043684e+00 -6.12406973e+00]\n",
      " [ 3.93109245e+00 -1.82391985e+01  1.16707517e+01 -3.96687001e-01\n",
      "   8.85825799e+00  1.89861649e+00  7.98063795e+00 -1.01932039e+00\n",
      "   7.43356544e+00 -1.50957268e+01]\n",
      " [-1.08071069e+01  7.25474004e+00 -3.91782562e-01 -2.28754171e+00\n",
      "  -1.79612295e+00  5.01725109e+00 -5.93343754e+00  5.10307597e+00\n",
      "  -9.15791849e+00 -4.07252043e+00]\n",
      " [ 9.84951672e+00  1.07125243e+01 -1.09715436e+01  8.38634747e+00\n",
      "  -1.03918232e+01  7.33023232e+00 -1.89881206e+01 -1.11711069e+01\n",
      "  -5.08972278e+00 -1.66485955e+00]\n",
      " [ 1.42361443e+01  9.03999174e+00  1.57546791e+01  1.20660790e+01\n",
      "  -2.82863552e+00 -2.66326884e+00  1.06897162e+01  4.03714310e-01\n",
      "  -1.56993672e+00 -1.33520272e+01]\n",
      " [-1.06460122e+00 -2.79099641e+01 -4.56117555e+00 -9.79890252e+00\n",
      "   6.92574348e+00 -4.78672356e+00 -3.29051549e+00  1.34710546e+01\n",
      "  -1.04906775e+01  3.16658895e+00]\n",
      " [-1.89526695e+01  8.97291174e-01  4.10265745e+00  8.59870972e+00\n",
      "  -8.98683193e+00  3.19656942e+00  3.18154200e+00 -1.92316341e-01\n",
      "   1.50016279e+00  4.63534322e+00]\n",
      " [ 3.97880425e+00 -9.96010889e+00 -1.19586151e+01  2.50598029e+01\n",
      "   1.91979229e+01 -1.39169388e+01  4.50217742e+00  6.27437083e+00\n",
      "   7.51337235e+00  1.40395436e+00]\n",
      " [-9.26871939e+00 -1.82420406e+00 -4.91125138e+00  1.34373116e+00\n",
      "  -2.68371304e+00 -1.31675626e+00  1.01855247e+01  1.23055820e+01\n",
      "  -1.18110317e+01 -4.59930104e+00]\n",
      " [-7.90799954e+00  1.22372221e+01 -5.93679025e-01  1.44898940e+01\n",
      "  -4.77580855e+00  2.59999942e-01 -1.34869645e+01  1.30253554e+01\n",
      "  -3.62612088e+00 -1.48515645e+01]\n",
      " [-5.92461285e+00 -2.30490794e+01 -3.18171727e-01  1.12487742e+00\n",
      "   2.88078167e+00  1.49810818e+01 -3.00976154e+00  8.07455917e+00\n",
      "   3.12238689e+00 -1.93321640e+00]\n",
      " [-2.07680202e+01  9.47501167e+00 -5.03973949e+00  1.79558917e-01\n",
      "  -1.27046078e+01  2.82995534e+00  1.08030817e+00  2.94176190e-01\n",
      "  -1.34793129e+00  1.04921829e+01]\n",
      " [ 9.66220863e+00  7.25916853e+00  3.32107876e+01 -6.00225330e+00\n",
      "  -3.79517516e+00 -1.01480369e+01  4.35986196e+00 -6.87487393e+00\n",
      "  -2.69836174e+01 -1.21333813e+01]\n",
      " [ 7.22518992e-01  1.00978733e+01 -1.55694156e+01 -6.12442128e+00\n",
      "  -1.39351805e+00 -7.28537489e+00  5.31163793e+00  4.00084198e-02\n",
      "   3.21265914e+00 -7.25214926e+00]\n",
      " [ 1.53653633e+01 -3.75008758e-03  1.29354962e+01 -4.38997664e+00\n",
      "   5.90039464e+00 -6.79383783e+00 -9.50909251e+00 -7.04350332e+00\n",
      "  -4.58666861e-01 -2.18733459e+00]\n",
      " [ 1.53920701e+01 -1.14870423e+01 -1.09033833e+01  1.70018815e+01\n",
      "   6.08783659e+00 -1.88141087e+01  4.97269099e+00  2.37332699e+00\n",
      "  -2.14444405e+01 -3.69562425e+00]\n",
      " [-1.74549518e-01  7.31402517e+00  9.54495667e+00  9.57467711e-01\n",
      "   1.03345080e+01 -1.46273275e+00 -8.57496825e+00 -9.34181843e+00\n",
      "   5.42645295e+00 -1.95816909e+01]\n",
      " [ 6.77807571e+00 -1.10657307e+01 -3.59224096e+00  5.05381903e+00\n",
      "   1.21794090e+01 -1.94068096e+01 -8.06178212e+00  4.90616924e-01\n",
      "  -5.96086335e+00  8.61623101e+00]\n",
      " [-2.08639057e+01  3.61801641e+00  4.25920177e+00  4.90803971e-01\n",
      "   1.10223673e+01 -1.22957425e+01  1.10861676e+01 -7.02920403e+00\n",
      "   7.25550518e+00 -3.24204219e+00]\n",
      " [ 8.14343129e+00  7.80469930e+00 -1.46405357e+01 -1.54491194e+00\n",
      "  -9.24323185e-01 -2.37875265e+00 -7.55662765e+00  1.85143789e+01\n",
      "   2.09096677e+00  1.55501599e+01]\n",
      " [-5.69148654e+00 -1.06179676e+01  1.32247779e+00 -5.63236604e+00\n",
      "   2.39014596e+01  2.45422849e+00  1.15259914e+01 -2.24235772e+00\n",
      "  -3.26061306e+00 -3.09114176e-01]\n",
      " [ 3.55717262e+00  8.49586845e+00 -1.22154015e+00 -6.80851574e+00\n",
      "  -1.06787658e+01 -7.66793627e-01  5.72962726e+00  4.57947076e+00\n",
      "  -1.78175491e-01 -6.00138799e+00]\n",
      " [ 1.46765263e+00  5.71804879e+00 -3.68176565e-01  1.12368489e+00\n",
      "  -1.50504326e+00  9.15499268e+00 -4.38200267e+00  1.85535621e+00\n",
      "   3.94428030e+00  7.25522558e+00]\n",
      " [ 1.49588477e+01  6.75453809e+00  5.99213235e+00 -1.47023709e+01\n",
      "   6.06403944e+00  2.29371761e+01 -8.30010986e+00 -1.01951985e+01\n",
      "  -2.14653842e+00  1.02124813e+01]\n",
      " [ 5.24750492e+00 -4.77124206e+00 -3.59901817e-01  1.03703898e+01\n",
      "   6.72619748e+00  2.42887697e+01  1.00568668e+01  3.53567216e+00\n",
      "   6.14726276e+00 -3.48984191e+00]\n",
      " [-9.77773002e+00  1.71957132e+00  4.90561044e+00 -1.39528303e+01\n",
      "  -5.22356465e+00 -3.69255902e+00  2.65642403e+00 -2.60466059e+00\n",
      "   4.45096710e+00  9.81122462e-01]\n",
      " [ 1.06032751e+01 -1.71116766e+01  1.65712464e+01  1.41767401e+01\n",
      "   5.03170861e-01  6.50323214e+00  6.06548400e+00 -7.37289628e+00\n",
      "   1.64665066e+00  7.78174179e+00]\n",
      " [ 3.09816759e+00  1.05132077e+01  9.49961101e-01  8.07509886e-01\n",
      "  -7.67803746e+00 -3.64538050e+00 -4.59717681e+00  1.70548352e+01\n",
      "   2.40505552e+00 -9.99426501e+00]\n",
      " [ 3.98598388e+00 -1.92003697e+00 -3.05376438e+01  4.79852371e+00\n",
      "  -1.55269878e+01  5.78464420e+00 -9.61263599e+00 -1.45832446e+01\n",
      "   4.94341651e+00 -1.49419377e+01]\n",
      " [-4.46699203e+00  2.04377395e+00  6.12232523e+00  7.44884536e+00\n",
      "  -3.62812886e-01 -8.32395348e+00  1.92381543e+01 -6.05981321e+00\n",
      "   1.80358898e+01 -4.52524973e+00]\n",
      " [ 1.16128569e+01  1.06996554e+01 -1.04553425e+01  3.55284507e+00\n",
      "   7.55392029e+00  7.00982122e+00 -1.98937450e+00  3.01960045e+00\n",
      "  -3.94689681e+00 -1.17181338e+01]\n",
      " [ 9.84012237e+00 -5.59681422e+00  1.37975819e+01  6.02450901e+00\n",
      "  -8.92646674e+00 -1.61198320e+00 -2.86384915e+00 -8.70887650e+00\n",
      "   5.01429590e+00 -4.78614074e+00]\n",
      " [ 1.63169151e+01  8.60891241e+00 -8.80189065e+00 -1.90005215e-01\n",
      "  -2.26760192e+00 -1.56450785e+01  9.31255679e+00  9.49808815e+00\n",
      "   9.25501215e+00 -4.56987858e+00]\n",
      " [ 1.06898597e+01 -2.09752935e+00  9.35147780e+00  1.81252782e+01\n",
      "   1.40109881e+00 -1.41914878e+01 -3.16901197e+00  6.40985866e+00\n",
      "   1.21987438e+01 -1.13379204e+01]\n",
      " [-1.90548298e+00  2.33339126e+00  4.34998324e+00  9.10423603e+00\n",
      "  -9.48439656e+00 -4.23478297e+00  1.00796648e+01  3.92334911e+00\n",
      "   4.48380651e+00  1.12532350e+01]\n",
      " [ 1.04053390e+00  5.28003422e+00 -3.14563862e+00 -1.34501002e+01\n",
      "  -1.29525789e+01  7.43205537e-01 -1.99560718e+00 -6.54603169e+00\n",
      "   3.18014296e+00 -8.90271552e+00]\n",
      " [ 1.11337266e+00 -1.95225583e-01 -8.39988915e+00 -2.29820588e+01\n",
      "   1.45652739e+01  3.16637236e+00 -2.66412594e+01 -4.26428618e+00\n",
      "   3.93787731e+00 -2.28140691e+00]\n",
      " [ 5.80330113e+00 -9.73267585e+00  1.75167729e+00 -5.34836927e-01\n",
      "  -1.83061987e+00 -2.21028902e+00  1.99759555e+00  9.32721414e+00\n",
      "  -5.30119800e+00 -4.07240024e+00]\n",
      " [ 1.60564992e+00 -1.20149976e+00  3.85602292e+00  7.18290736e+00\n",
      "   1.29118890e+01 -1.16444148e+00 -2.27729800e+01 -6.96245395e-01\n",
      "   3.53870427e+00 -1.86955017e+00]] \n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(n_instances, n_input)\n",
    "    y = np.random.choice([0,1], size=(n_instances,), replace=True)\n",
    "    Y = np.zeros(shape=(n_instances, n_output))\n",
    "    Y[np.arange(0, len(y)),y]=1\n",
    "    return X, Y\n",
    "\n",
    "X, Y = init_toy_data()\n",
    "\n",
    "print(X, \"\\n\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 10)\n",
      "(30, 10)\n",
      "(70, 2)\n",
      "(30, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the random seed to make sure your output is same with ours\n",
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED) #CHANGED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our code will follow the following steps in building a two layer network:\n",
    "\n",
    "```\n",
    "**(a)** set up the size of our network \n",
    "**(b)** initialize weights variables \n",
    "**(c)** initialize our data placeholders\n",
    "\n",
    "start loop\n",
    "**(d)** do forward propagation\n",
    "**(e)** get the predictions\n",
    "**(f)** calculate for the loss (predictions - ground truth)\n",
    "**(g)** do backward propagation to update/optimize the weight variables (see **(c)**) : \n",
    "end loop\n",
    "\n",
    "```\n",
    "Notes on ```**(a)**```: <br>\n",
    "We already did this a few cells before\n",
    "\n",
    "Notes on ```**(b)**```: <br>\n",
    "TensorFlow has `variables`, which are normal variables we use in coding. By declaring our weights as Tensorflow variables, we are saying that this value should be updated during gradient descent.\n",
    "\n",
    "Notes on ```**(c)**```: <br>\n",
    "TensorFlow uses `placeholders` for inputs that we will feed in later (our data X). We do not put in the values of X when designing our model (we do that in training), but we should know the shape of our input.\n",
    "\n",
    "Notes on ```**(g)**```: <br>\n",
    "We'll use Adam as our optimizer.\n",
    "\n",
    "\n",
    "```**(d)** and **(b)**``` will be in their separate methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```**(b)**``` **initialize our weights**\n",
    "\n",
    "We will just initialize our weights simply following a normal distribution formula. This will suit our small dataset.\n",
    "\n",
    "Remember, we have two sets of weights for our two layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.normal?\n",
    "tf.Variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_weights(n_input, n_hidden, n_output):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    # Declare the size of the weights\n",
    "    # TODO : Initialize the weight using tf.random.normal, give it a std dev of 0.1   \n",
    "    #        Set the seed parameter to 42 so we will get the same result\n",
    "    ### START CODE HERE ###\n",
    "    #REMOVED---------------------------------\n",
    "    #tf.random.set_seed(45);\n",
    "    #w1 = tf.random.normal([n_input, n_output], stddev=0.1)\n",
    "    #w2 = tf.random.normal([n_hidden, n_output], stddev=0.1)\n",
    "    #----------------------------------------\n",
    "    w1 = tf.random.normal(shape=(n_input, n_hidden), stddev=0.1, seed=42)\n",
    "    w2 = tf.random.normal(shape=(n_hidden, n_output), stddev=0.1, seed=42)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Here, we will declare our weights as tf.Variables\n",
    "    # Inputs in TensorFlow that are variables will be updated during gradient descent\n",
    "    # If we want our weights to be updated, we have to declare them as  variables\n",
    "    weights = {\n",
    "        'w1': tf.Variable(w1),\n",
    "        'w2': tf.Variable(w2)\n",
    "    }\n",
    "\n",
    "    return weights\n",
    "\n",
    "weights = init_weights(n_input, n_hidden, n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sanity check below). Printing out the values in TensorFlow is not as easy before. Here, we have to \"run\" the whole computation graph to see the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w1': array([[ 0.13148774, -0.01542157,  0.09113878, -0.07991441, -0.01087529,\n",
      "         0.02843679,  0.07661625, -0.06211289,  0.09974318,  0.02195926],\n",
      "       [-0.10798668, -0.19874832,  0.15603696, -0.01690948, -0.13780487,\n",
      "        -0.02980155,  0.08410001,  0.09225498, -0.04837923, -0.16971248],\n",
      "       [ 0.00953648, -0.01067538, -0.17839275,  0.17145915, -0.11761415,\n",
      "         0.11488946,  0.04505377, -0.08172406,  0.09519397,  0.13913034],\n",
      "       [ 0.01586228,  0.01048807,  0.11451166, -0.10479503,  0.02145801,\n",
      "         0.02575068, -0.09960396,  0.055202  , -0.08870138,  0.34017047],\n",
      "       [ 0.03429196, -0.04389471, -0.15277438, -0.08316436,  0.03434062,\n",
      "        -0.27989998, -0.09340011, -0.04060407,  0.12880985,  0.12897399],\n",
      "       [-0.03741515, -0.14218496, -0.01342447,  0.10253482, -0.32489178,\n",
      "         0.18755275,  0.08539876,  0.06199211, -0.02141522,  0.20731623],\n",
      "       [ 0.01948587, -0.03687083,  0.03235206,  0.17784952,  0.06728835,\n",
      "         0.11926039,  0.00293781,  0.13695234, -0.08442237,  0.01464307],\n",
      "       [-0.2167099 , -0.05404393, -0.13511175, -0.11596807, -0.027453  ,\n",
      "        -0.07361402,  0.03883214, -0.02485349, -0.0798556 , -0.04665783],\n",
      "       [ 0.11286335, -0.11491549,  0.02331937, -0.0759256 ,  0.03863364,\n",
      "        -0.14149378, -0.17656256, -0.0665978 , -0.05536773, -0.14172284],\n",
      "       [ 0.11051842,  0.12187224, -0.06621953,  0.07448888, -0.11173725,\n",
      "         0.07188234,  0.05128387, -0.07941299,  0.1241065 , -0.08387285]],\n",
      "      dtype=float32), 'w2': array([[ 0.13148774, -0.01542157],\n",
      "       [ 0.09113878, -0.07991441],\n",
      "       [-0.01087529,  0.02843679],\n",
      "       [ 0.07661625, -0.06211289],\n",
      "       [ 0.09974318,  0.02195926],\n",
      "       [-0.10798668, -0.19874832],\n",
      "       [ 0.15603696, -0.01690948],\n",
      "       [-0.13780487, -0.02980155],\n",
      "       [ 0.08410001,  0.09225498],\n",
      "       [-0.04837923, -0.16971248]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# start session\n",
    "#---------------session = tf.compat.v1.Session()  REMOVED\n",
    "session = tf.Session()\n",
    "# initialize\n",
    "#---------------init = tf.compat.v1.global_variables_initializer()  REMOVED\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)\n",
    "\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# call weights (which wil then call init_weights)\n",
    "print(session.run(weights))\n",
    "\n",
    "# close the session\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check: **\n",
    "\n",
    "\n",
    "Your result should exactly match the one below (because we set the random seed)\n",
    "```\n",
    "{'w1': array([[ 0.13148773, -0.01542157,  0.09113878, -0.07991441, -0.01087529,\n",
    "         0.02843679,  0.07661625, -0.06211289,  0.09974318,  0.02195926],\n",
    "       [-0.10798668, -0.19874832,  0.15603696, -0.01690948, -0.13780487,\n",
    "        -0.02980155,  0.08410001,  0.09225498, -0.04837923, -0.1697125 ],\n",
    "       [ 0.00953648, -0.01067538, -0.17839274,  0.17145914, -0.11761414,\n",
    "         0.11488946,  0.04505377, -0.08172406,  0.09519397,  0.13913034],\n",
    "       [ 0.01586228,  0.01048807,  0.11451166, -0.10479503,  0.02145801,\n",
    "         0.02575068, -0.09960396,  0.055202  , -0.08870138,  0.34017047],\n",
    "       [ 0.03429196, -0.04389471, -0.15277438, -0.08316436,  0.03434062,\n",
    "        -0.27989998, -0.09340011, -0.04060407,  0.12880985,  0.12897399],\n",
    "       [-0.03741515, -0.14218496, -0.01342447,  0.10253482, -0.32489178,\n",
    "         0.18755275,  0.08539876,  0.06199211, -0.02141522,  0.20731623],\n",
    "       [ 0.01948587, -0.03687083,  0.03235206,  0.17784952,  0.06728835,\n",
    "         0.11926039,  0.00293781,  0.13695234, -0.08442237,  0.01464307],\n",
    "       [-0.2167099 , -0.05404393, -0.13511175, -0.11596807, -0.027453  ,\n",
    "        -0.07361402,  0.03883214, -0.02485349, -0.0798556 , -0.04665783],\n",
    "       [ 0.11286335, -0.11491551,  0.02331937, -0.0759256 ,  0.03863364,\n",
    "        -0.14149378, -0.17656256, -0.0665978 , -0.05536773, -0.14172284],\n",
    "       [ 0.11051842,  0.12187224, -0.06621953,  0.07448888, -0.11173725,\n",
    "         0.07188234,  0.05128387, -0.07941299,  0.1241065 , -0.08387285]], dtype=float32), 'w2': array([[ 0.13148773, -0.01542157],\n",
    "       [ 0.09113878, -0.07991441],\n",
    "       [-0.01087529,  0.02843679],\n",
    "       [ 0.07661625, -0.06211289],\n",
    "       [ 0.09974318,  0.02195926],\n",
    "       [-0.10798668, -0.19874832],\n",
    "       [ 0.15603696, -0.01690948],\n",
    "       [-0.13780487, -0.02980155],\n",
    "       [ 0.08410001,  0.09225498],\n",
    "       [-0.04837923, -0.1697125 ]], dtype=float32)}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```**(c)**``` **forward propagation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_forwardprop(X, W1, W2):\n",
    "    '''\n",
    "    Input :\n",
    "    X : a matrix with shape (?, n_input) or (?, D), the input we will feed to our model; ? means arbitrary\n",
    "    W1: a matrix with shape (n_input, n_hidden), weights of our first layer\n",
    "    W2: a matrix with shape (n_hidden, n_output), weights of our 2nd layer\n",
    "    \n",
    "    Output:\n",
    "    z2 : a matrix with shape (n_instances,n_output), the score of the hidden layer \n",
    "    a2 : a matrix with shape (n_instances, n_output), the activation of the output \n",
    "    '''\n",
    "    # TODO : Initialize the weight using tf.random.normal, give it a std dev of 0.1   \n",
    "    #        Set the seed parameter to 42 so we will get the same result\n",
    "    # HINT : Use tf.matmul for calculating the scores\n",
    "    #        Use tf.nn.sigmoid for the sigmoid gate\n",
    "    #        Make sure the matrix are ordered to be conformable\n",
    "    ### START CODE HERE ###\n",
    "    z1 =  tf.matmul(X, W1)    # 1st layer score\n",
    "    a1 =  tf.nn.sigmoid(z1)    # 1st layer activation\n",
    "    #------------CHANGED\n",
    "    #z2 =  tf.matmul(X, W2)     # 2nd layer score\n",
    "    #--------------------\n",
    "    z2 =  tf.matmul(a1, W2)     # 2nd layer score\n",
    "    a2 =  tf.nn.sigmoid(z2)    # 2nd layer activation\n",
    "    ### END CODE HERE ###\n",
    "    return a2, z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?tf.nn.sigmoid\n",
    "#?tf.matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Sanity check below)\n",
    "\n",
    "Building a computational graph typically includes:\n",
    "1. Desigining our computational graph\n",
    "2. Running our computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 [[0.524076   0.44027126]] \n",
      "z2 [[ 0.09637842 -0.24006137]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Design our computation graph\n",
    "# design our X as a placeholder\n",
    "#X = tf.compat.v1.placeholder(\"float32\", shape=[None, n_input])  REMOVED\n",
    "X = tf.placeholder(\"float32\", shape=[None, n_input])\n",
    "\n",
    "weights = init_weights(n_input, n_hidden, n_output)\n",
    "forwardprop = do_forwardprop(X, weights['w1'], weights['w2'])\n",
    "\n",
    "# Step 2: Run the computation graph\n",
    "#session = tf.compat.v1.Session()  REMOVED\n",
    "#init = tf.compat.v1.global_variables_initializer()  REMOVED\n",
    "session = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "session.run(init)\n",
    "# feed the values of our X tf.placeholder, this will be fed in when forwardprop is callled\n",
    "# here, the value given to feed_dict's X should match the size of the tf.placeholder X\n",
    "a2, z2 = session.run(forwardprop, feed_dict={X:[[1,2,3,4,5,6,7,8,9,10]]})\n",
    "print(\"a2\", a2, \"\\nz2\",z2) # let's see the result\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check: **\n",
    "\n",
    "Your result should be :\n",
    "```\n",
    "a2 [[ 0.52407598  0.44027126]] \n",
    "z2 [[ 0.09637842 -0.24006137]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pause**\n",
    "\n",
    "The code above is a lot to take in. Try changing the code a little to see how it works. \n",
    "\n",
    "You can try:\n",
    "- Creating a new method that only does adding/multiplying\n",
    "- Changing the size of the placeholder and see what happens\n",
    "- Removing parts of the code and see what happens\n",
    "\n",
    "We believe that this is crucial before you will understand the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's piece the whole thing together\n",
    "\n",
    "We will follow the pseudo code written above before:\n",
    "\n",
    "```\n",
    "**(a)** set up the size of our network \n",
    "**(b)** initialize weights variables \n",
    "**(c)** initialize our data placeholders\n",
    "\n",
    "start loop\n",
    "**(d)** do forward propagation\n",
    "**(e)** get the predictions\n",
    "**(f)** calculate for the loss (predictions - ground truth)\n",
    "**(g)** do backward propagation to update/optimize the weight variables (see **(c)**) : \n",
    "end loop\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, train accuracy = 48.57%, test accuracy = 46.67%\n",
      "Epoch = 2, train accuracy = 50.00%, test accuracy = 46.67%\n",
      "Epoch = 3, train accuracy = 50.00%, test accuracy = 46.67%\n",
      "Epoch = 4, train accuracy = 51.43%, test accuracy = 46.67%\n",
      "Epoch = 5, train accuracy = 51.43%, test accuracy = 46.67%\n",
      "Epoch = 6, train accuracy = 57.14%, test accuracy = 46.67%\n",
      "Epoch = 7, train accuracy = 60.00%, test accuracy = 53.33%\n",
      "Epoch = 8, train accuracy = 60.00%, test accuracy = 50.00%\n",
      "Epoch = 9, train accuracy = 62.86%, test accuracy = 53.33%\n",
      "Epoch = 10, train accuracy = 64.29%, test accuracy = 53.33%\n",
      "Epoch = 11, train accuracy = 68.57%, test accuracy = 50.00%\n",
      "Epoch = 12, train accuracy = 70.00%, test accuracy = 50.00%\n",
      "Epoch = 13, train accuracy = 70.00%, test accuracy = 53.33%\n",
      "Epoch = 14, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 15, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 16, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 17, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 18, train accuracy = 71.43%, test accuracy = 56.67%\n",
      "Epoch = 19, train accuracy = 71.43%, test accuracy = 56.67%\n",
      "Epoch = 20, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 21, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 22, train accuracy = 71.43%, test accuracy = 56.67%\n",
      "Epoch = 23, train accuracy = 71.43%, test accuracy = 60.00%\n",
      "Epoch = 24, train accuracy = 70.00%, test accuracy = 56.67%\n",
      "Epoch = 25, train accuracy = 71.43%, test accuracy = 56.67%\n",
      "Epoch = 26, train accuracy = 72.86%, test accuracy = 60.00%\n",
      "Epoch = 27, train accuracy = 74.29%, test accuracy = 60.00%\n",
      "Epoch = 28, train accuracy = 74.29%, test accuracy = 60.00%\n",
      "Epoch = 29, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 30, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 31, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 32, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 33, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 34, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 35, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 36, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 37, train accuracy = 74.29%, test accuracy = 56.67%\n",
      "Epoch = 38, train accuracy = 75.71%, test accuracy = 56.67%\n",
      "Epoch = 39, train accuracy = 75.71%, test accuracy = 56.67%\n",
      "Epoch = 40, train accuracy = 75.71%, test accuracy = 56.67%\n",
      "Epoch = 41, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 42, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 43, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 44, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 45, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 46, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 47, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 48, train accuracy = 77.14%, test accuracy = 56.67%\n",
      "Epoch = 49, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 50, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 51, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 52, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 53, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 54, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 55, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 56, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 57, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 58, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 59, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 60, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 61, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 62, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 63, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 64, train accuracy = 78.57%, test accuracy = 56.67%\n",
      "Epoch = 65, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 66, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 67, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 68, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 69, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 70, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 71, train accuracy = 80.00%, test accuracy = 56.67%\n",
      "Epoch = 72, train accuracy = 82.86%, test accuracy = 56.67%\n",
      "Epoch = 73, train accuracy = 82.86%, test accuracy = 56.67%\n",
      "Epoch = 74, train accuracy = 82.86%, test accuracy = 56.67%\n",
      "Epoch = 75, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 76, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 77, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 78, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 79, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 80, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 81, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 82, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 83, train accuracy = 82.86%, test accuracy = 60.00%\n",
      "Epoch = 84, train accuracy = 84.29%, test accuracy = 60.00%\n",
      "Epoch = 85, train accuracy = 84.29%, test accuracy = 60.00%\n",
      "Epoch = 86, train accuracy = 85.71%, test accuracy = 60.00%\n",
      "Epoch = 87, train accuracy = 85.71%, test accuracy = 60.00%\n",
      "Epoch = 88, train accuracy = 85.71%, test accuracy = 60.00%\n",
      "Epoch = 89, train accuracy = 85.71%, test accuracy = 56.67%\n",
      "Epoch = 90, train accuracy = 85.71%, test accuracy = 56.67%\n",
      "Epoch = 91, train accuracy = 87.14%, test accuracy = 56.67%\n",
      "Epoch = 92, train accuracy = 87.14%, test accuracy = 56.67%\n",
      "Epoch = 93, train accuracy = 87.14%, test accuracy = 56.67%\n",
      "Epoch = 94, train accuracy = 87.14%, test accuracy = 56.67%\n",
      "Epoch = 95, train accuracy = 87.14%, test accuracy = 56.67%\n",
      "Epoch = 96, train accuracy = 88.57%, test accuracy = 56.67%\n",
      "Epoch = 97, train accuracy = 88.57%, test accuracy = 56.67%\n",
      "Epoch = 98, train accuracy = 88.57%, test accuracy = 56.67%\n",
      "Epoch = 99, train accuracy = 88.57%, test accuracy = 56.67%\n",
      "Epoch = 100, train accuracy = 90.00%, test accuracy = 56.67%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #------------session = tf.compat.v1.Session()  REMOVED\n",
    "    # loss_over_time will log down the loss per epoch\n",
    "    loss_over_time = np.array([])\n",
    "\n",
    "    # **(a)** set up the size of our network \n",
    "    # TODO : Place in the sizes of each layer in our network; copy paste as necessary\n",
    "    #        Set the hidden layer size to 10\n",
    "    ### START CODE HERE ###\n",
    "    n_input  = 10      # Number of input nodes: # features and 1 bias\n",
    "    n_hidden = 10      # Number of hidden nodes\n",
    "    n_output = 2      # Number of outputs \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # **(b)** initialize weights variables \n",
    "    # TODO : Initialize the weights\n",
    "    ### START CODE HERE ###\n",
    "    weights = init_weights(n_input, n_hidden, n_output)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # **(c)** initialize our data placeholders\n",
    "    # TODO : Initialize out data placeholders\n",
    "    ### START CODE HERE ###\n",
    "    #----------------------------------CHANGED\n",
    "    #X = tf.compat.v1.placeholder(\"float32\", shape=[None, n_input])      # 1st parameter is None because to accommodate multiple instanced\n",
    "    #y = tf.compat.v1.placeholder(\"float32\", shape=[None, n_input])      # 1st parameter is None because to accommodate multiple instanced\n",
    "    #----------------------------------------------------\n",
    "    \n",
    "    X =  tf.placeholder(\"float32\", shape=[None, n_input])     # 1st parameter is None because to accommodate multiple instanced\n",
    "    y =  tf.placeholder(\"float32\", shape=[None, n_output])\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # **(d)** do forward propagation\n",
    "    # TODO : Call the forward propagation code, and pass the proper weight values\n",
    "    ### START CODE HERE ###\n",
    "    a2, z2 = do_forwardprop(X, weights['w1'], weights['w2'])\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # **(e)** get the predictions\n",
    "    predict  = tf.argmax(a2, axis=1)\n",
    "\n",
    "    # **(f)** calculate for the loss (predictions - ground truth)\n",
    "    # tf.reduce_mean gets the mean\n",
    "    # tf.nn.sigmoid_cross_entropy_with_logits gets the raw score (z2) and computes for the probability\n",
    "    #       it also computes for the loss\n",
    "    loss      = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=z2))\n",
    "    \n",
    "    # **(g)** do backward propagation to update/optimize the weight variables\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss) \n",
    "    \n",
    "#     '''\n",
    "    if optimizer_fn == \"momentum\":\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=1e-3, momentum=0.9).minimize(loss) \n",
    "    elif optimizer_fn == \"rmsprop\":\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(loss) \n",
    "    elif optimizer_fn == \"adam\":\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss) \n",
    "    elif optimizer_fn == \"gd\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer().minimize(loss)\n",
    "#     '''\n",
    "        \n",
    "    # Finally, we run the computational graph\n",
    "    #-------------------------CHANGED\n",
    "    #session = tf.compat.v1.Session()\n",
    "    #init = tf.compat.v1.global_variables_initializer()\n",
    "    #-------------------------------------------------\n",
    "    session = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # Train with each example\n",
    "        for i in range(len(X_train)):\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: X_train[i: i + 1], y: y_train[i: i + 1]})\n",
    "\n",
    "        # add current loss to loss_over_time\n",
    "        loss_over_time = np.append(loss_over_time, l)\n",
    "        \n",
    "        train_accuracy = np.mean(np.argmax(y_train, axis=1) == session.run(predict, feed_dict={X: X_train, y: y_train}))\n",
    "        test_accuracy  = np.mean(np.argmax(y_test, axis=1) == session.run(predict, feed_dict={X: X_test, y: y_test}))\n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "    weights = session.run(weights)\n",
    "    session.close()\n",
    "    \n",
    "    return loss_over_time, weights\n",
    "\n",
    "# loss_over_time, weights = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pause**\n",
    "\n",
    "It may seem like all the gradients that we did in the lecture wasn't included in the code above. But, reading the code will reveal that the **Optimizer function minimizes the loss, and does the backward propagation of gradients automatically for us.**\n",
    "\n",
    "**About what `main()` returns:**\n",
    "<br>\n",
    "**`loss_over_time`** : logs of the loss, we expect this to lower every epoch <br>\n",
    "**`weights`** : the weights of our network (now with two layers `w1` and `w2`\n",
    "(Understand what the code is doing as a whole)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** \n",
    "\n",
    "You should see 100 epochs printed out. The train accuracy gets better over time, eventually reaching 90+% accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, len(loss_over_time))\n",
    "plt.plot(x, loss_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the difference in optimization speed if we try out an optimizer on **Momentum** and **RMSProp** instead.\n",
    "\n",
    "Modify the code above to accept an optimizer parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO : Modify the main method to accept an optimizer parameter\n",
    "###        Create an if-else statement to change the optimizer function based on this parameter\n",
    "###        For momentum and rmsprop, set a learning rate of 1e-3\n",
    "###        Additionally for momentum, set the momentum hyperparameter to 0.9\n",
    "\n",
    "momentum_loss, momentum_weight = main(\"momentum\")\n",
    "rmsprop_loss, rmsprop_weight = main(\"rmsprop\")\n",
    "adam_loss, adam_weight = main(\"adam\")\n",
    "\n",
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all the optimizer's loss graph together\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(momentum_loss, label=\"momentum\")\n",
    "plt.plot(rmsprop_loss, label=\"rmsprop\")\n",
    "plt.plot(adam_loss,label=\"adam\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check: **\n",
    "You should see all three lines decaying. In this particular example, RMSProp reaches lower loss faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the network again for the iris dataset. This is a real dataset that distinguishes whether a flower is an *Iris Setosa, Iris Versicolour, Iris Virginica* given the flower's *sepal width, sepal length, petal width, and petal length*. You can read more about it here: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "\n",
    "# Read iris data from sklearn\n",
    "iris   = datasets.load_iris()\n",
    "data   = iris[\"data\"]\n",
    "target = iris[\"target\"]\n",
    "\n",
    "# Add a column of 1s for bias\n",
    "N, M  = data.shape\n",
    "X = np.ones((N, M + 1))\n",
    "X[:, 1:] = data\n",
    "\n",
    "# Convert into one-hot vectors\n",
    "num_labels = len(np.unique(target))\n",
    "Y = np.eye(num_labels)[target] \n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out how `y` looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our train from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO : Set the test size to 0.33, and the random seed to RANDOM_SEED (42)\n",
    "### START CODE HERE ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (100, 5)\n",
      "y_train shape : (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape :\", X_train.shape)\n",
    "print(\"y_train shape :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a 2 layer network\n",
    "Modify the network we built above to fit the iris dataset. Account for any changes, like in the number of features and the number of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TODO : Create the model from before, redesign it to fit the iris dataset\n",
    "### HINT : Change the sigmoid_cross_entropy to softmax_cross_entropy\n",
    "def main():\n",
    "    ### START CODE HERE ###\n",
    "    # loss_over_time will log down the loss per epoch\n",
    "    loss_over_time = np.array([])\n",
    "\n",
    "    # **(a)** set up the size of our network \n",
    "    # TODO : Place in the sizes of each layer in our network; copy paste as necessary\n",
    "    #        Set the hidden layer size to 10\n",
    "    ### START CODE HERE ###\n",
    "    n_input  = 5      # Number of input nodes: # features and 1 bias\n",
    "    n_hidden = 5      # Number of hidden nodes\n",
    "    n_output = 3      # Number of outputs \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # **(b)** initialize weights variables \n",
    "    # TODO : Initialize the weights\n",
    "    ### START CODE HERE ###\n",
    "    weights = init_weights(n_input, n_hidden, n_output)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # **(c)** initialize our data placeholders\n",
    "    # TODO : Initialize out data placeholders\n",
    "    ### START CODE HERE ###\n",
    "    #----------------------------------CHANGED\n",
    "    #X = tf.compat.v1.placeholder(\"float32\", shape=[None, n_input])      # 1st parameter is None because to accommodate multiple instanced\n",
    "    #y = tf.compat.v1.placeholder(\"float32\", shape=[None, n_input])      # 1st parameter is None because to accommodate multiple instanced\n",
    "    #----------------------------------------------------\n",
    "    \n",
    "    X =  tf.placeholder(\"float32\", shape=[None, n_input])     # 1st parameter is None because to accommodate multiple instanced\n",
    "    y =  tf.placeholder(\"float32\", shape=[None, n_output])\n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # **(d)** do forward propagation\n",
    "    # TODO : Call the forward propagation code, and pass the proper weight values\n",
    "    ### START CODE HERE ###\n",
    "    a2, z2 = do_forwardprop(X, weights['w1'], weights['w2'])\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # **(e)** get the predictions\n",
    "    predict  = tf.argmax(a2, axis=1)\n",
    "\n",
    "    # **(f)** calculate for the loss (predictions - ground truth)\n",
    "    # tf.reduce_mean gets the mean\n",
    "    # tf.nn.sigmoid_cross_entropy_with_logits gets the raw score (z2) and computes for the probability\n",
    "    #       it also computes for the loss\n",
    "    loss      = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=z2))\n",
    "    \n",
    "    # **(g)** do backward propagation to update/optimize the weight variables\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss) \n",
    "    \n",
    "#     '''\n",
    "    if optimizer_fn == \"momentum\":\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=1e-3, momentum=0.9).minimize(loss) \n",
    "    elif optimizer_fn == \"rmsprop\":\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(loss) \n",
    "    elif optimizer_fn == \"adam\":\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(loss) \n",
    "    elif optimizer_fn == \"gd\":\n",
    "        optimizer = tf.train.GradientDescentOptimizer().minimize(loss)\n",
    "#     '''\n",
    "        \n",
    "    # Finally, we run the computational graph\n",
    "    #-------------------------CHANGED\n",
    "    #session = tf.compat.v1.Session()\n",
    "    #init = tf.compat.v1.global_variables_initializer()\n",
    "    #-------------------------------------------------\n",
    "    session = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # Train with each example\n",
    "        for i in range(len(X_train)):\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: X_train[i: i + 1], y: y_train[i: i + 1]})\n",
    "\n",
    "        # add current loss to loss_over_time\n",
    "        loss_over_time = np.append(loss_over_time, l)\n",
    "        \n",
    "        train_accuracy = np.mean(np.argmax(y_train, axis=1) == session.run(predict, feed_dict={X: X_train, y: y_train}))\n",
    "        test_accuracy  = np.mean(np.argmax(y_test, axis=1) == session.run(predict, feed_dict={X: X_test, y: y_test}))\n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "\n",
    "    weights = session.run(weights)\n",
    "    session.close()\n",
    "    \n",
    "    return loss_over_time, weights\n",
    "\n",
    "loss_over_time, weights = main()\n",
    "\n",
    "# momentum_loss, momentum_weight = main(\"momentum\")\n",
    "# rmsprop_loss, rmsprop_weight = main(\"rmsprop\")\n",
    "# adam_loss, adam_weight = main(\"adam\")\n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "You should get a train and test accuracy ~97%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "x = np.arange(0, len(loss_over_time))\n",
    "plt.plot(x, loss_over_time)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try these out:\n",
    "\n",
    "- See if you can get a good accuracy with the spiral and circle dataset (see below)\n",
    "- Create a 3 layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 : Circle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_circle_data(num_points):\n",
    "    r = np.random.uniform(0,2,num_points)\n",
    "    theta = np.random.uniform(0,2*np.pi,num_points)\n",
    "    inner_circle = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "    \n",
    "    r = np.random.uniform(5,7,num_points)\n",
    "    theta = 2*np.pi*np.arange(num_points)/num_points\n",
    "    outer_circle = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "\n",
    "    X = np.concatenate((inner_circle,outer_circle),axis=0)\n",
    "    y = np.concatenate((np.ones(num_points),np.zeros(num_points)),axis=0)\n",
    "    \n",
    "    randIdx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randIdx)\n",
    "    \n",
    "    X = X[randIdx]\n",
    "    y = y[randIdx].astype(int)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def generate_dummy_spiral_data(num_points, num_spiral):\n",
    "    r = np.random.uniform(-0.1, 0.1,num_points) + 5*np.arange(num_points)/num_points\n",
    "    theta = np.random.uniform(-0.1, 0.1,num_points) + 2*np.pi*1.25*np.arange(num_points)/num_points\n",
    "    spiral = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "    y = np.ones(num_points)\n",
    "\n",
    "    for i in range(1,num_spiral+1):\n",
    "        r = np.random.uniform(-0.1, 0.1,num_points) + 5*np.arange(num_points)/num_points\n",
    "        theta = np.random.uniform(-0.1, 0.1,num_points) + 2*np.pi*1.25*np.arange(num_points)/num_points + 2*i*np.pi/num_spiral\n",
    "        tmp_spiral = np.array([r*np.sin(theta), r*np.cos(theta)]).T\n",
    "\n",
    "        spiral = np.concatenate((spiral,tmp_spiral),axis=0)\n",
    "        if i % 2 == 1:\n",
    "            y = np.concatenate((y,np.zeros(num_points)),axis=0)\n",
    "        else:\n",
    "            y = np.concatenate((y,np.ones(num_points)),axis=0)\n",
    "\n",
    "    randIdx = np.arange(spiral.shape[0])\n",
    "    np.random.shuffle(randIdx)\n",
    "\n",
    "    X = spiral[randIdx]\n",
    "    y = y[randIdx].astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_circle,y_circle = generate_dummy_circle_data(100)\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_circle[:,0],X_circle[:,1],c=y_circle)\n",
    "\n",
    "X_spiral,y_spiral = generate_dummy_spiral_data(100,4)\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_spiral[:,0],X_spiral[:,1],c=y_spiral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output = 2\n",
    "Y_circle = np.zeros(shape=(len(y_circle),n_output))\n",
    "Y_circle[np.arange(0, len(y_circle)),y_circle] =1\n",
    "\n",
    "Y_spiral = np.zeros(shape=(len(y_spiral),n_output))\n",
    "Y_spiral[np.arange(0, len(y_spiral)),y_spiral] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
